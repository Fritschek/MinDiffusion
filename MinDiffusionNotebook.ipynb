{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69bdd7ff",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a2ee65-2ef7-49b2-a846-63d41819591b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_swiss_roll, make_s_curve, make_moons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc7fd67",
   "metadata": {},
   "source": [
    "## Parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0686a8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "schedules = ['linear', 'quad', 'sigmoid', 'cosine']\n",
    "num_steps, start, end = 100, 1e-4, 5e-3\n",
    "mode = \"DDIM\" # DDPM or DDIM\n",
    "iterations = 3000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a46e483-e6b2-40cf-8633-1b9c9e249ac3",
   "metadata": {},
   "source": [
    "# Data Preparation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafd9285-8cc2-461b-964f-b15ae12e0cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "datasets = {\n",
    "    \"s_curve\": make_s_curve(10**4, noise=0.1)[0][:, [0, 2]] / 10.0,\n",
    "    \"swiss_roll\": make_swiss_roll(10**4, noise=0.1)[0][:, [0, 2]] / 10.0,\n",
    "    \"moons\": make_moons(10**4, noise=0.1)[0]\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(8, 2))\n",
    "for ax, (name, data) in zip(axes, datasets.items()):\n",
    "    ax.scatter(*data.T, alpha=0.5, color='white', edgecolor='green', s=3)\n",
    "    ax.axis('off')\n",
    "    ax.set_title(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37563ed-889a-441a-bd07-c15702ec6574",
   "metadata": {},
   "source": [
    "# Diffusion in General:\n",
    "\n",
    "Starting with a sample $x_0 \\sim q(x_0)$, it has a forward process which adds Gaussian noise over $T$ time steps. The process can be defined as\n",
    "> $q(x_{1:T}|x_0) = \\Pi_{i=1}^T q(x_{t}|x_{t-1})$ with $q(x_t|x_{t-1})=\\mathcal{N}(x_t; \\sqrt{1-\\beta_t}x_{t-1};\\beta_t\\mathbf{I})$ and $\\{\\beta_t\\in (0,1)\\}_{t=1}^T$\n",
    "\n",
    "\n",
    "The sequence of $\\beta_t$`s can be chosen in different ways, and optimized, we refer to this as scheduler. Here are some ways (note that the end point should be such that $\\beta_t\\ll1$):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73c58a7-519f-48e2-9ebc-046eb691fb95",
   "metadata": {},
   "source": [
    "# Sequence Scheduler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031f4e54-6ec7-4991-b806-27df4eddcaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_beta_seq(schedule='linear', n_timesteps=1000, start=1e-4, end=1e-2):\n",
    "    if schedule == 'linear':\n",
    "        betas = torch.linspace(start, end, n_timesteps)\n",
    "    elif schedule == \"quad\":\n",
    "        betas = torch.linspace(start ** 0.5, end ** 0.5, n_timesteps) ** 2\n",
    "    elif schedule == \"sigmoid\":\n",
    "        betas = torch.linspace(-6, 6, n_timesteps)\n",
    "        betas = torch.sigmoid(betas) * (end - start) + start\n",
    "    elif schedule == \"cosine\":\n",
    "        timesteps = torch.linspace(0, 1, n_timesteps)\n",
    "        betas = (1-(torch.cos(timesteps * torch.pi) + 1) / 2)\n",
    "        betas = betas * (end - start) + start \n",
    "    return betas\n",
    "\n",
    "\n",
    "# Plotting the sequences\n",
    "fig, axes = plt.subplots(1, 4, figsize=(8, 2))\n",
    "for ax, schedule in zip(axes, schedules):\n",
    "    betas = make_beta_seq(schedule=schedule, n_timesteps=num_steps, start=start, end=end)\n",
    "    ax.plot(betas, label=schedule.capitalize())\n",
    "    ax.set_title(f'{schedule.capitalize()} Schedule')\n",
    "    if schedule == 'linear':\n",
    "        ax.set_ylabel('Beta Value')\n",
    "    ax.set_xlabel('Timestep')\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becbc7cb-4de8-49b3-9487-ac9ba2125b7d",
   "metadata": {},
   "source": [
    "# Forward Process:\n",
    "#### (Make the samples noisy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f51451-2073-4857-98e3-c0489f70c222",
   "metadata": {},
   "source": [
    "Note that one can sample directly from\n",
    "> $q(x_t|x_0)=\\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha}_t}x_{0};(1-\\bar{\\alpha}_t)\\mathbf{I})$\n",
    "\n",
    "without going through the whole chain of the process. For that we define $\\alpha_t=(1-\\beta_t)$ and $\\bar{\\alpha_t}=∏_{t=1}^T\\alpha_t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137ff724-c8fa-4917-ad2e-bd09d9070b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "betas = make_beta_seq(schedule=\"linear\", n_timesteps=num_steps, start=start, end=end)\n",
    "alphas = 1 - betas\n",
    "alphas_bar = torch.cumprod(alphas, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6bfd37-0703-4c62-88e8-adebad18612c",
   "metadata": {},
   "source": [
    "Due to the Gaussian property that for $\\epsilon\\sim\\mathcal{N}(0,1)$, we have $\\mu+\\sigma\\epsilon \\sim \\mathcal{N}(\\mu, \\sigma^2)$, we can generate the noisy samples of $x_T$ (or any other $x_t$) by utilizing the Gaussian property $\\mu x_0+\\sigma\\epsilon$, plugging in $\\alpha$ we get:\n",
    "\n",
    ">$q(x_t|x_0) = \\sqrt{\\bar{\\alpha_t}} x_0 + (1-\\bar{\\alpha_t})*\\epsilon$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af514f1-7e3c-4b1f-a272-5f69f305d820",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_x(x_0, t, noise): #q(x_t | x_0)\n",
    "    coeff1 = torch.sqrt(alphas_bar)[t].unsqueeze(-1)\n",
    "    coeff2 = torch.sqrt(1 - alphas_bar)[t].unsqueeze(-1)\n",
    "    return coeff1 * x_0 + coeff2 * noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2455cba-2bbd-4dd7-9c22-47361eda643e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 10, figsize=(20, 2))\n",
    "for i in range(10):\n",
    "    x_0 = torch.from_numpy(datasets['swiss_roll'])\n",
    "    noise = torch.randn_like(x_0)\n",
    "    q_i = q_x(x_0=x_0, t=torch.tensor([i * 10]), noise=noise)\n",
    "    axs[i].scatter(q_i[:, 0], q_i[:, 1],color='white',edgecolor='green', s=3)\n",
    "    axs[i].set_axis_off(); axs[i].set_title('$q(\\mathbf{x}_{'+str(i*10)+'})$')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9077dc89-c180-4730-aaf3-7075b00c6e51",
   "metadata": {},
   "source": [
    "# Reverse process / sampling:\n",
    "\n",
    "Now we want to reverse the process, and get $q(x_{t-1}|x_{t})$, i.e., given the further noisy sample in the step, we want to produce the less noisy sample from before. If we can do that, we can sample from clean noise, representing a random noisy $x_T$, reverse the whole chain, and arrive at a new data sample $x_0^\\dagger$. Here we utilize, that all $\\beta_t \\ll 1$, then the distirbution class of the reverse step is the same as the forward step, i.e., Gaussian. Now we need to learn a *parametrized* approximation\n",
    "> $p_{\\theta}(x_{t-1}|x_{t})=\\mathcal{N}(x_{t};\\mu_{\\theta}(x_t),\\sigma_{\\theta}(x_t))$ of the reverse step.\n",
    "\n",
    "After a lengthy derivation, the paper [Ho et al., DDPM, https://arxiv.org/abs/2006.11239 ] arrives at the mean estimate\n",
    ">$\\mu_{\\theta}=\\frac{1}{\\sqrt{\\bar{\\alpha}_t}} \\left(x_t - \\frac{b_t}{\\sqrt{1-\\bar{\\alpha}_t}} \\epsilon_{\\theta}(x_t) \\right)$.\n",
    "\n",
    "Here, the $\\epsilon_{\\theta}$ term stands for the noise and is learned as a prediction of the noise $\\epsilon$ from $x_t$\n",
    "\n",
    ">$L = \\mathbb{E}_{t\\in U[1:T],x_0}\\left[|| \\epsilon - \\epsilon_{\\theta}(x_t) ||^2\\right]$.\n",
    "\n",
    "Breaking this down, $\\epsilon_{\\theta}(\\cdot)$ will be represented by a parametrized neural network:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a20e76c",
   "metadata": {},
   "source": [
    "### DDIM Sampling:\n",
    "[Song et al., DDIM, https://arxiv.org/abs/2010.02502]\n",
    "\n",
    ">$x_{t-1}=\\sqrt{\\frac{\\bar{\\alpha}_{t-1}}{\\bar{\\alpha}_t}} \\left(x_t - \\sqrt{1-\\bar{\\alpha}_t} \\epsilon_{\\theta}(x_t) \\right)+\\sqrt{1-\\bar{\\alpha}_{t-1}-\\sigma_t^2} \\epsilon_{\\theta}(x_t)+\\sigma_t\\epsilon_{\\theta}(x_t)$\n",
    "\n",
    "where $\\sigma_t$ is a factor to choose the style of the model. For $\\sigma_t:=0$ the sampling becomes deterministic, yielding the known DDIM function:\n",
    "\n",
    ">$x_{t-1}=\\sqrt{\\frac{\\bar{\\alpha}_{t-1}}{\\bar{\\alpha}_t}} \\left(x_t - \\sqrt{1-\\bar{\\alpha}_t} \\epsilon_{\\theta}(x_t) \\right)+\\sqrt{1-\\bar{\\alpha}_{t-1}} \\epsilon_{\\theta}(x_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eaa2f1c-45fc-403e-bf9c-18ca1e0f448a",
   "metadata": {},
   "source": [
    "# Diffusion (epsilon) Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3a99b9-00d7-49d3-8cc1-0658d70fb787",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalLinear(nn.Module):\n",
    "    def __init__(self, num_in, num_out, n_steps):\n",
    "        super().__init__()\n",
    "        self.num_out = num_out\n",
    "        self.lin = nn.Linear(num_in, num_out)\n",
    "        self.embed = nn.Embedding(n_steps, num_out)\n",
    "        nn.init.uniform_(self.embed.weight)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        out = self.lin(x)\n",
    "        gamma = self.embed(y).view(-1, self.num_out)\n",
    "        return gamma * out\n",
    "\n",
    "class ConditionalModel(nn.Module):\n",
    "    def __init__(self, n_steps):\n",
    "        super().__init__()\n",
    "        first_layer = ConditionalLinear(2, 128, n_steps)\n",
    "        middle_layers = [ConditionalLinear(128, 128, n_steps) for _ in range(2)]\n",
    "        self.layers = nn.ModuleList([first_layer] + middle_layers)\n",
    "        self.final = nn.Linear(128, 2)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        for layer in self.layers:\n",
    "            x = F.softplus(layer(x, y))\n",
    "        return self.final(x)\n",
    "\n",
    "model = ConditionalModel(num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcf6066",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772499c3-1901-4514-8f4c-4b3261835834",
   "metadata": {},
   "source": [
    "# Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc520d93-68f8-48a3-9e78-00b978e544f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample with the help of our epsilon model\n",
    "def p_sample(model, x_t, t, mode='DDIM'):\n",
    "    t = torch.tensor([t])\n",
    "    factor = (betas[t] / torch.sqrt(1-alphas_bar)[t]).unsqueeze(-1)\n",
    "    eps_theta = model(x_t, t)\n",
    "    mu_theta = (x_t - (factor * eps_theta)) / torch.sqrt(alphas)[t].unsqueeze(-1)\n",
    "    \n",
    "    if mode == 'DDIM':\n",
    "        if t > 0: \n",
    "            x_prev = torch.sqrt(alphas_bar[t-1]/alphas_bar[t]) * (x_t - torch.sqrt(1.0 - alphas_bar[t]) * eps_theta)\n",
    "            x_prev += torch.sqrt(1.0 - alphas_bar[t-1]) * eps_theta\n",
    "            return x_prev\n",
    "        else:\n",
    "            return x_t\n",
    "    else:  # DDPM mode\n",
    "        z = torch.randn_like(x_t)\n",
    "        sigma_t = betas[t].sqrt().unsqueeze(-1)\n",
    "        return mu_theta + sigma_t * z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1ffe6a-c9ec-4997-8cec-a181194002e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_sample_loop(model, shape, num_steps, mode='DDIM'):\n",
    "    x_t = torch.randn(shape)\n",
    "    x_seq = [x_t]\n",
    "    \n",
    "    if mode == 'DDIM':\n",
    "        timesteps = range(0, num_steps, 2) # start, end, step_size\n",
    "    else:\n",
    "        timesteps = range(num_steps)\n",
    "\n",
    "    for t in reversed(timesteps):\n",
    "        x_t = p_sample(model, x_t, t, mode)\n",
    "        x_seq.append(x_t)\n",
    "\n",
    "    return x_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be12978-f161-4728-85bb-7a08ab3d4647",
   "metadata": {},
   "source": [
    "# Loss Term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51ee99d-c41c-425e-ad03-427e326e40c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates the loss term L\n",
    "\n",
    "def noise_estimation_loss(model, x_0):\n",
    "    batch_size = x_0.shape[0]\n",
    "    # Select a random step for each example\n",
    "    t = torch.randint(0, num_steps, size=(batch_size,))\n",
    "\n",
    "    # model input\n",
    "    noise = torch.randn_like(x_0)\n",
    "    x_t = q_x(x_0, t, noise)\n",
    "    eps_theta = model(x_t, t)\n",
    "    return (noise - eps_theta).square().mean() # =: Loss L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70d186a-2c07-4ad5-b7f0-f3a85920326f",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d220d9-4ff0-4ad8-90c8-addda71591fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tensor = torch.tensor(datasets['swiss_roll'], dtype=torch.float32)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for t in range(iterations):\n",
    "    permutation = torch.randperm(data_tensor.shape[0])\n",
    "    for i in range(0, data_tensor.shape[0], batch_size):\n",
    "        optimizer.zero_grad()\n",
    "        indices = permutation[i:i + batch_size]\n",
    "        batch_x = data_tensor[indices]\n",
    "        loss = noise_estimation_loss(model, batch_x)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.)\n",
    "        optimizer.step()\n",
    "\n",
    "    if (t % (iterations//10) == 0):\n",
    "        print(f\"Iteration: {t+1} Loss: {loss}\")\n",
    "\n",
    "        x_seq = p_sample_loop(model, data_tensor.shape, num_steps, mode)\n",
    "        \n",
    "        # Determine indices for plotting based on mode\n",
    "        if mode == 'DDIM':\n",
    "            num_samples_to_plot = 10\n",
    "            num_samples = len(x_seq)\n",
    "            if num_samples < num_samples_to_plot:\n",
    "                plot_indices = np.arange(num_samples)\n",
    "            else:\n",
    "                plot_indices = np.linspace(0, num_samples-1, num=num_samples_to_plot, dtype=int)\n",
    "        else:  # DDPM\n",
    "            plot_indices = [0] + list(range(10, 101, 10))\n",
    "\n",
    "        # Create subplots based on the number of indices\n",
    "        fig, axs = plt.subplots(1, len(plot_indices), figsize=(20, 2))\n",
    "\n",
    "        # Plot the samples\n",
    "        for i, idx in enumerate(plot_indices):\n",
    "            cur_x = x_seq[idx].detach()\n",
    "            axs[i].scatter(cur_x[:, 0], cur_x[:, 1], color='white', edgecolor='gray', s=3)\n",
    "            axs[i].set_axis_off()\n",
    "            axs[i].set_title(f'$q(\\\\mathbf{{x}}_{{{idx}}})$')\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b5bba1-6ca1-4b7d-8ffb-a0d7aa25405b",
   "metadata": {},
   "source": [
    "We can now sample with the formula $x_{t-1}= \\mu_{\\theta} + \\sigma_t z$, where \\mu_{\\theta} is given by the formular above and the resulting neural network output, $\\sigma_t$ is just $\\sqrt{\\beta}$ and $z\\sim \\mathbb{N}(0,I)$. THis is done in p_sample, and then we need to loop over the whole sequence p_sample_loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfde684",
   "metadata": {},
   "source": [
    "### Save/Load the Model Parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a7b8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model parameters\n",
    "# torch.save(model.state_dict(), 'model_state_dict_DDIM.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa60ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model state dictionary into \"model\"\n",
    "model.load_state_dict(torch.load('model_state_dict_DDIM.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7162a824",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tensor = torch.tensor(datasets['swiss_roll'], dtype=torch.float32)\n",
    "x_seq = p_sample_loop(model, data_tensor.shape, num_steps, mode)\n",
    "sample = x_seq[len(x_seq)-1].detach()\n",
    "plt.scatter(sample[:, 0], sample[:, 1], color='white', edgecolor='green', s=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08b4f87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
